# Crime Prediction & Analysis (Vancouver) - ML Pipeline

End-to-end, reproducible ML project that predicts **crime type** from time/location/context using the Vancouver Police open data (via Kaggle) by Supervised Machine Learning - Classification. 
Includes data download, cleaning/feature engineering, modeling, evaluation, error analysis, and tests.

---

## Features
- Clean and process raw crime data
- Cyclical feature engineering for time
- Categorical encoding and numeric scaling
- Train ML models (Random Forest, Logistic Regression, XGBoost)
- Predict and save crime types for new data
- EDA and error analysis notebooks

---

## Concepts Used

### 1. Supervised Machine Learning - Classification
- **Target Variable:** `TYPE` (crime type)  
- **Input Features:** Time, location, and other categorical features  
### 2. Feature Engineering
- **Cyclical Features:** Transformed time-based features using sine and cosine functions:  
  - `month_sin`, `month_cos`  
  - `hour_sin`, `hour_cos`  
- **Categorical Encoding:** One-hot encoding applied to `NEIGHBOURHOOD`  
- **Numeric Scaling:** StandardScaler applied to `latitude` and `longitude`  
### 3. Pipeline
- Utilizes **`ColumnTransformer` + `Pipeline`** to ensure consistent transformation of both training and test datasets  
### 4. Modeling
- **Primary Model:** Random Forest (default)  
- **Alternative Options:** Logistic Regression, XGBoost  
- Supports **multi-class classification**  
### 5. Evaluation
- Predictions are saved as CSV files  
- Summaries include counts of each predicted class  
- **Error Analysis:** Jupyter notebook available to inspect correct vs. incorrect predictions  
### 6. Testing
- Unit tests for feature engineering and data cleaning:  
  - `tests/test_features.py`  
  - `tests/test_data.py`

---

## Tools & Libraries
- Python 3, pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, joblib, pyyaml, kaggle API

---

## üì¶ Project Structure

- data/ ‚Äì Contains all datasets
- raw/ ‚Äì Original/raw CSVs (e.g., downloaded from Kaggle)
- interim/ ‚Äì Cleaned/intermediate data used during processing
- processed/ ‚Äì Final train/test datasets used for modeling

- models/ ‚Äì Stores trained models and pipeline artifacts
- model.pkl ‚Äì Trained machine learning model

- artifacts/ ‚Äì Feature pipelines and transformers (e.g., pipeline.joblib)

- notebooks/ ‚Äì Jupyter notebooks for exploration and analysis
- 01_eda.ipynb ‚Äì Exploratory Data Analysis (plots, counts, distributions)
- 02_error_analysis.ipynb ‚Äì Evaluate predictions and inspect errors

- scripts/ ‚Äì Python scripts for workflow automation
- download_data.py ‚Äì Download dataset from Kaggle if not already present
- make_dataset.py ‚Äì Clean raw data, create train/test CSVs
- train.py ‚Äì Train model and save pipeline
- evaluate.py ‚Äì Predict on test data, save results, summarize predictions
- utils.py ‚Äì Logging, config loading, and utility functions

- src/ ‚Äì Source code for modular functionality
- data.py ‚Äì Data loading and cleaning functions (basic_clean, load_raw)
- features.py ‚Äì Feature engineering, pipeline creation (add_derived_features, build_feature_pipeline)

- tests/ ‚Äì Unit tests for code validation
- test_data.py ‚Äì Tests for data cleaning functions
- test_features.py ‚Äì Tests for feature engineering and pipelines

- .gitignore ‚Äì Ignore compiled files, cache, models, and processed data
- config.yaml ‚Äì Configuration file for paths, model parameters, feature settings
- requirements.txt ‚Äì Python dependencies (pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, joblib, pyyaml, kaggle, tabulate)

---

## üóÇ Dataset

**Kaggle**: Crime in Vancouver (2003‚Äì2017). This dataset includes columns like TYPE, YEAR, MONTH, DAY, HOUR, MINUTE, HUNDRED_BLOCK, NEIGHBOURHOOD, X, Y, Latitude, Longitude. 

Kaggle API notes : Create ~/.kaggle/kaggle.json with your API token (from Kaggle ‚Üí Account ‚Üí Create New Token).

Or manually download the CSV from Kaggle and drop it into data/raw/ as crime.csv.
The downloader script will detect it and skip the API call.

---

## 8Ô∏è‚É£ Nice-to-Haves / Next Steps

- Add metrics like **accuracy**, **F1-score**, and **confusion matrix** in evaluation  
- Add **geospatial plots** using latitude/longitude (e.g., heatmaps, hexbin)  
- Support **XGBoost** and **hyperparameter tuning**  
- Add **interactive dashboard** for predictions or EDA  
- More sophisticated features: **day-of-week**, **holiday indicator**, **weather**, **population density**

---

## üîß Troubleshooting

No Kaggle credentials? Place crime.csv in data/raw/ and re-run.

Memory errors? Decrease train.sample_frac in config.yaml.

Plot issues on headless servers? Set MPLBACKEND=Agg.

---

## Quick Start
```bash
# Setup environment
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Download dataset
python -m scripts.download_data

# Prepare train/test datasets
python -m scripts.make_dataset

# Train model
python -m scripts.train

# Predict & evaluate
python -m scripts.evaluate

#Inspect EDA & Errors
Open notebooks: notebooks/01_eda.ipynb and notebooks/02_error_analysis.ipynb
